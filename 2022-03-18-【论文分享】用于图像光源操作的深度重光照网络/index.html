<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="baidu-site-verification" content="codeva-6wmKWl5hmx" />
<meta name="bytedance-verification-code" content="GcOf/MpQ8shZ5Hh/2hvr" />
<meta name="google-site-verification" content="wivqPhuFdISMEKkFZjOWHPYJGSvd716d9R7Bwy2Jj-A" />
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-4636539228226058"
     crossorigin="anonymous"></script>
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.1.1">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.0/css/all.min.css" integrity="sha256-yIDrPSXHZdOZhAqiBP7CKzIwMQmRCJ8UeB8Jo17YC4o=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">

<script class="next-config" data-name="main" type="application/json">{"hostname":"www.vvbuys.com","root":"/","images":"/images","scheme":"Muse","darkmode":false,"version":"8.19.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12},"copycode":{"enable":false,"style":null},"fold":{"enable":false,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"Searching...","empty":"We didn't find any results for the search: ${query}","hits_time":"${hits} results found in ${time} ms","hits":"${hits} results found"}}</script><script src="/js/config.js"></script>

    <meta name="description" content="​		本文介绍ECCV Workshop 2020年的《Deep Relighting Networks for Image Light Source Manipulation》，作者主要是来自香港理工大学，相关代码已经发布在 https:&#x2F;&#x2F;github.com&#x2F;WangLiwen1994&#x2F;DeepRelight。该文章的方法参加了“AIM Image Relighting Challenge">
<meta property="og:type" content="article">
<meta property="og:title" content="「论文分享」用于图像光源操作的深度重光照网络">
<meta property="og:url" content="https://www.vvbuys.com/2022-03-18-%E3%80%90%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E3%80%91%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/index.html">
<meta property="og:site_name" content="VVbugs Blog">
<meta property="og:description" content="​		本文介绍ECCV Workshop 2020年的《Deep Relighting Networks for Image Light Source Manipulation》，作者主要是来自香港理工大学，相关代码已经发布在 https:&#x2F;&#x2F;github.com&#x2F;WangLiwen1994&#x2F;DeepRelight。该文章的方法参加了“AIM Image Relighting Challenge">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/DRN%E7%9A%84%E6%9E%B6%E6%9E%84.png">
<meta property="og:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/DRN%E7%9A%84%E5%9C%BA%E6%99%AF%E9%87%8D%E5%BB%BA.png">
<meta property="og:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E5%8F%8D%E5%90%91%E6%8A%95%E5%BD%B1%E5%9D%97%E7%9A%84%E5%9B%BE%E7%A4%BA.png">
<meta property="og:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E5%8F%8D%E5%90%91%E6%8A%95%E5%BD%B1%E5%9D%97%E7%9A%84%E5%85%AC%E5%BC%8F.png">
<meta property="og:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E6%9B%9D%E5%85%89%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95.png">
<meta property="og:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E9%98%B4%E5%BD%B1%E5%85%88%E9%AA%8C%E4%BC%B0%E8%AE%A1%E7%BD%91%E7%BB%9C.png">
<meta property="og:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E9%87%8D%E6%B8%B2%E6%9F%93%E6%A8%A1%E5%9D%97.png">
<meta property="og:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E4%B8%8D%E5%90%8C%E7%BB%93%E6%9E%84%E7%9A%84%E6%AF%94%E8%BE%83.png">
<meta property="og:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E5%AE%9A%E9%87%8F%E5%AF%B9%E6%AF%94%E7%BB%93%E6%9E%9C.png">
<meta property="og:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E5%AE%9A%E6%80%A7%E5%AF%B9%E6%AF%94%E7%BB%93%E6%9E%9C.png">
<meta property="article:published_time" content="2022-03-18T00:00:00.000Z">
<meta property="article:modified_time" content="2024-02-02T01:00:55.431Z">
<meta property="article:author" content="vvbuys">
<meta property="article:tag" content="论文分享">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://www.vvbuys.com/img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/DRN%E7%9A%84%E6%9E%B6%E6%9E%84.png">


<link rel="canonical" href="https://www.vvbuys.com/2022-03-18-%E3%80%90%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E3%80%91%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"en","comments":true,"permalink":"https://www.vvbuys.com/2022-03-18-%E3%80%90%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E3%80%91%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/","path":"2022-03-18-【论文分享】用于图像光源操作的深度重光照网络/","title":"「论文分享」用于图像光源操作的深度重光照网络"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>「论文分享」用于图像光源操作的深度重光照网络 | VVbugs Blog</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">VVbugs Blog</p>
      <i class="logo-line"></i>
    </a>
      <p class="site-subtitle" itemprop="description">standalone Linux lover</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="Search" role="button">
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a></li><li class="menu-item menu-item-sitemap"><a href="/sitemap.xml" rel="section"><i class="fa fa-sitemap fa-fw"></i>Sitemap</a></li><li class="menu-item menu-item-commonweal"><a href="/404/" rel="section"><i class="fa fa-heartbeat fa-fw"></i>Commonweal 404</a></li>
  </ul>
</nav>




</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%91%98%E8%A6%81"><span class="nav-number">1.</span> <span class="nav-text">摘要</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%96%B9%E6%B3%95"><span class="nav-number">2.</span> <span class="nav-text">方法</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1%E3%80%81%E9%87%8D%E5%85%89%E7%85%A7%E4%BC%B0%E8%AE%A1%EF%BC%88Assumption-of-Relighting%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">1、重光照估计（Assumption of Relighting）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2%E3%80%81%E5%9C%BA%E6%99%AF%E9%87%8D%E5%BB%BA%EF%BC%88Scene-Reconversion%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">2、场景重建（Scene Reconversion）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E5%8F%8D%E5%90%91%E6%8A%95%E5%BD%B1%E5%9D%97%EF%BC%88Back-Projection-Block%EF%BC%89"><span class="nav-number">2.2.1.</span> <span class="nav-text">（1）反向投影块（Back-Projection Block）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E5%8D%8A%E7%9B%91%E7%9D%A3%E9%87%8D%E5%BB%BA%EF%BC%88Semi-supervised-reconversion%EF%BC%89"><span class="nav-number">2.2.2.</span> <span class="nav-text">（2）半监督重建（Semi-supervised reconversion）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%883%EF%BC%89%E5%AF%B9%E6%8A%97%E5%AD%A6%E4%B9%A0%EF%BC%88Adversarial-Learning%EF%BC%89"><span class="nav-number">2.2.3.</span> <span class="nav-text">（3）对抗学习（Adversarial Learning）</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3%E3%80%81%E9%98%B4%E5%BD%B1%E5%85%88%E9%AA%8C%E4%BC%B0%E8%AE%A1%EF%BC%88Shadow-Prior-Estimation%EF%BC%89"><span class="nav-number">2.3.</span> <span class="nav-text">3、阴影先验估计（Shadow Prior Estimation）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4%E3%80%81%E9%87%8D%E6%B8%B2%E6%9F%93%E6%A8%A1%E5%9D%97"><span class="nav-number">2.4.</span> <span class="nav-text">4、重渲染模块</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C"><span class="nav-number">3.</span> <span class="nav-text">实验</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1%E3%80%81%E6%95%B0%E6%8D%AE%E9%9B%86"><span class="nav-number">3.1.</span> <span class="nav-text">1、数据集</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2%E3%80%81%E8%AE%AD%E7%BB%83%E7%AD%96%E7%95%A5"><span class="nav-number">3.2.</span> <span class="nav-text">2、训练策略</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3%E3%80%81%E8%AE%AD%E7%BB%83%E8%AE%BE%E7%BD%AE"><span class="nav-number">3.3.</span> <span class="nav-text">3、训练设置</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4%E3%80%81%E8%AF%84%E4%BB%B7%E6%96%B9%E6%B3%95"><span class="nav-number">3.4.</span> <span class="nav-text">4、评价方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5%E3%80%81%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C"><span class="nav-number">3.5.</span> <span class="nav-text">5、消融实验</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%88A%EF%BC%89Pix2Pix%E5%92%8CShadAdv%E7%9A%84%E6%AF%94%E8%BE%83"><span class="nav-number">3.5.0.1.</span> <span class="nav-text">（A）Pix2Pix和ShadAdv的比较</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%88B%EF%BC%89%E5%8F%8D%E6%8A%95%E5%BD%B1%E6%A8%A1%E5%9D%97%E7%9A%84%E6%95%88%E6%9E%9C"><span class="nav-number">3.5.0.2.</span> <span class="nav-text">（B）反投影模块的效果</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%88C%EF%BC%89%E9%87%8D%E5%85%89%E7%85%A7%E5%81%87%E8%AE%BE%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="nav-number">3.5.0.3.</span> <span class="nav-text">（C）重光照假设的影响</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#6%E3%80%81%E5%AF%B9%E6%AF%94%E5%AE%9E%E9%AA%8C"><span class="nav-number">3.6.</span> <span class="nav-text">6、对比实验</span></a></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">vvbuys</p>
  <div class="site-description" itemprop="description">Share some post and some issue for linux program</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">265</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">36</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>

        </div>
      </div>
    </div>

    
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="en">
    <link itemprop="mainEntityOfPage" href="https://www.vvbuys.com/2022-03-18-%E3%80%90%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E3%80%91%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="vvbuys">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="VVbugs Blog">
      <meta itemprop="description" content="Share some post and some issue for linux program">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="「论文分享」用于图像光源操作的深度重光照网络 | VVbugs Blog">
      <meta itemprop="description" content="">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          「论文分享」用于图像光源操作的深度重光照网络
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">Posted on</span>

      <time title="Created: 2022-03-18 00:00:00" itemprop="dateCreated datePublished" datetime="2022-03-18T00:00:00+00:00">2022-03-18</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">Edited on</span>
      <time title="Modified: 2024-02-02 01:00:55" itemprop="dateModified" datetime="2024-02-02T01:00:55+00:00">2024-02-02</time>
    </span>

  
</div>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><p>​		本文介绍ECCV Workshop 2020年的《Deep Relighting Networks for Image Light Source Manipulation》，作者主要是来自香港理工大学，相关代码已经发布在 <a target="_blank" rel="noopener" href="https://github.com/WangLiwen1994/DeepRelight%E3%80%82%E8%AF%A5%E6%96%87%E7%AB%A0%E7%9A%84%E6%96%B9%E6%B3%95%E5%8F%82%E5%8A%A0%E4%BA%86%E2%80%9CAIM">https://github.com/WangLiwen1994/DeepRelight。该文章的方法参加了“AIM</a> Image Relighting Challenge - Track 1: any to one”，任务目标是在给定任何类型照明下的图像，估计在给出特定光源（色温为 4500k，光方向为东）下的重照明图像。 </p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>​			操纵给定图像的光源是一项有趣的任务，在各种应用中都很有用，包括摄影和电影摄影。现有的方法通常需要额外的信息，如场景的几何结构，这可能不适用于大多数图像<strong>。在本文中，我们用公式表示单图像重照明任务，并提出了一种新的深度重照明网络（DRN）</strong>，该网络由三部分组成：</p>
<p>​			1）场景重建，其目的是通过深度自动编码网络显示主要场景结构。</p>
<p>​			2）阴影先验估计，通过对抗性学习，从新的灯光方向预先确定灯光效果。</p>
<p>​			3）重新渲染，将主要结构与重建的阴影视图结合起来，形成目标光源下所需的估计图像。</p>
<p>​			实验结果表明，该方法在定性和定量上都优于其他方法。具体而言，本文提出的DRN在2020年ECCV大会的“AIM2020-任何对一重新照明挑战”中实现了最佳峰值信噪比（PSNR）。</p>
<h1 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h1><p>​		该文提出的深度重照明网络（DRN）如下图所示：</p>
<p>![](&#x2F;img-post&#x2F;论文分享&#x2F;2022-03-18-用于图像光源操作的深度重光照网络&#x2F;DRN的架构.png)</p>
<p><img src="/../img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/DRN%E7%9A%84%E6%9E%B6%E6%9E%84.png"></p>
<p>​		DRN由三部分组成：场景重建、阴影先验估计和重新渲染器。 首先，输入图像在场景重建网络中处理以去除照明的影响，这从输入图像中提取固有结构。 同时，另一个分支侧重于光照效果的变化，根据目标光源重新投射阴影。 接下来，重新渲染器部分感知光照效果并在结构信息的支持下重新绘制图像。 场景重建和阴影先验估计网络都具有类似的深度自动编码器结构，这是一种Pix2Pix网络增强的变体。</p>
<h2 id="1、重光照估计（Assumption-of-Relighting）"><a href="#1、重光照估计（Assumption-of-Relighting）" class="headerlink" title="1、重光照估计（Assumption of Relighting）"></a>1、重光照估计（Assumption of Relighting）</h2><p>​		该文章讨论Any-To-One的单图像重光照任务。假设输入图片为$\mathbf{X} \in \mathbb{R}^{\mathrm{H} \times \mathrm{W} \times 3}$，其可能是处于任意光源$\Phi$下的照明效果，该任务旨在给定目标光源$\Psi$重新渲染出新的图像。根据 2004年的《Retinex processing for automatic image enhancement》和2018年的《Deep retinex decomposition for lowlight enhancement》的Retinex 理论 ， 输入图像可以写成：<br>$$<br>\mathbf{X}&#x3D;L_{\Phi}(\mathbf{S})<br>$$<br>​		其中$\mathbf{S}$是在不同光照条件下不可改变的固有场景信息， $L_{\Phi}()$是定义的一个照明操作 ，它用以提供在光源$\Phi$下的全局照明和场景 $\mathbf{S}$产生的阴影效果。</p>
<p>​		图像重光照的公式表达如下：<br>$$<br>\mathbf{Y}&#x3D;L_{\Psi}\left(L_{\Phi}^{-1}(\mathbf{X})\right)<br>$$<br>​		其整体可以分为以下两步操作：</p>
<p>​		（1）场景重建操作：$L_{\Phi}^{-1}()$。该步是从输入图像$\mathbf{X}$中恢复结构信息$\mathbf{S}$，目的是去除原有的光照效果，该步骤的关键是去除阴影</p>
<p>​		（2）重光照操作：$L_{\Psi}()$。该步是用目标光源$\Psi$重新进行照明操作$L_{\Psi}()$，从而得到新的图像$\mathbf{Y}$，该步骤的关键是增加阴影。</p>
<p>​			由于单图像重光照没有额外的几何信息输入，所以重光照操作的难度更大。该文称其所提出的方法不是直接找到某个重光照操作$L_{\Psi}()$，而是寻找一个将光照效果（主要是阴影）从输入图像迁移到目标图像的转移操作$L_{(\Phi \rightarrow \Psi)}(\mathbf{X})$，这显著降低了重新绘制阴影的难度，最后使用一个重渲染过程$P\left( ·\right)$来结合场景信息和光照效果。整个过程可以表示为：<br>$$<br>\hat{\mathbf{Y}}&#x3D;P\left(L_{\Phi}^{-1}(\mathbf{X}), L_{(\Phi \rightarrow \Psi)}(\mathbf{X})\right)<br>$$</p>
<h2 id="2、场景重建（Scene-Reconversion）"><a href="#2、场景重建（Scene-Reconversion）" class="headerlink" title="2、场景重建（Scene Reconversion）"></a>2、场景重建（Scene Reconversion）</h2><p>​		场景重建的目的是从图像$\mathbf{X}$中提取固有的结构信息$\mathbf{S}$，从而去除光照效果。 该网络采用带有跳跃连接的自动编码器结构 [22] 来提取浅层特征，具备一个与 Pix2Pix 方法（ 2017年《Image-to-image translation with conditional adversarial networks》 ）类似的自动编码器结构，其中 9 个残差块（2016年的《Deep residual learning for image recognition》）图中的“ResBlocks”）用于消除光照效果。</p>
<p>​		![](&#x2F;img-post&#x2F;论文分享&#x2F;2022-03-18-用于图像光源操作的深度重光照网络&#x2F;DRN的场景重建.png)</p>
<p><img src="/../img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/DRN%E7%9A%84%E5%9C%BA%E6%99%AF%E9%87%8D%E5%BB%BA.png"></p>
<p>​		【下采样】首先， 输入图像$Input$被进行4次下采样操作（图中的“DBP”）来找到场景的判别特征（编码）。 每次下采样过程后，通道都会加倍，以尽可能多地保留信息。 这些特征具有大的感受野，其中包含许多有益于光照估计和操纵的全局信息。</p>
<p>​		【上采样】然后，经过上采样的特征图经过4次上采样操作（图中的“UBP”）映射回原始图片大小，然后通过跳跃连接的浅层特征丰富它。 特征图与由卷积层作用的特征选择过程进一步聚合，将通道从 64 减少到 32（“Conv.”（灰色矩形））， 最后输入重新渲染器流程。</p>
<h3 id="（1）反向投影块（Back-Projection-Block）"><a href="#（1）反向投影块（Back-Projection-Block）" class="headerlink" title="（1）反向投影块（Back-Projection Block）"></a>（1）反向投影块（<strong>Back-Projection Block</strong>）</h3><p>​		该文没有通过常规的池化或步幅卷积过程来对特征进行下采样或者上采样，而是采用反向投影块，通过残差来弥补丢失的信息。</p>
<p>​	![](&#x2F;img-post&#x2F;论文分享&#x2F;2022-03-18-用于图像光源操作的深度重光照网络&#x2F;反向投影块的图示.png)</p>
<p><img src="/../img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E5%8F%8D%E5%90%91%E6%8A%95%E5%BD%B1%E5%9D%97%E7%9A%84%E5%9B%BE%E7%A4%BA.png"></p>
<p>​		 如上图所示，下采样反投影 (DBP) 和上采样反投影 (UBP) 块由在输入空间和潜在空间之间映射信息的编码和解码操作组成。 以 DBP 块为例，它首先将输入（X）映射到潜在的空间 (Z) 通过编码过程（E1，由步长卷积层作用，过滤器大小为 3 × 3，步长为 2，填充为 1）。 然后，解码器（D2，由滤波器大小为 4 × 4，步长为 2，填充为 1 的反卷积层）将其映射回输入空间（ ˆX）以计算差异（残差，RX &#x3D; X − ˆX  ）。 残差被编码（E2，由滤波器大小为 3×3，步幅为 2，填充为 1 的步幅卷积层作用）到潜在空间 RZ 以修复潜在代码（ˆZ &#x3D; Z+RZ）。 </p>
<p>​		数学上，DBP 和 UBP可以写成：</p>
<p>​		![](&#x2F;img-post&#x2F;论文分享&#x2F;2022-03-18-用于图像光源操作的深度重光照网络&#x2F;反向投影块的公式.png)</p>
<p><img src="/../img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E5%8F%8D%E5%90%91%E6%8A%95%E5%BD%B1%E5%9D%97%E7%9A%84%E5%85%AC%E5%BC%8F.png"></p>
<p>​	</p>
<h3 id="（2）半监督重建（Semi-supervised-reconversion）"><a href="#（2）半监督重建（Semi-supervised-reconversion）" class="headerlink" title="（2）半监督重建（Semi-supervised reconversion）"></a>（2）半监督重建（<strong>Semi-supervised reconversion</strong>）</h3><p>​		<strong>完全监督学习和半监督学习的区别？</strong>完全监督学习使用明确定义的标签来训练网络，半监督学习无法得到明确定义的真实标签，只能使用接近真实的参考标签来训练网络。		</p>
<p>​		从单张图像中提取的场景信息很难定义，所以我们无法通过明确定义的真实图像来完全监督网络，只能通过人工观察到的图像来半监督网络。该文方法使用的场景重建网络使用单张图片中相应的无阴影图像作为半监督的真实图片来训练网络，这种无阴影图像可能包含来自固有场景结构的冗余信息。</p>
<p>​		该文采用曝光融合方法来获取场景的无阴影图片，该方法曾被广泛用于改善在不均匀光照条件下捕获的图像的动态范围。曝光融合方法将多张曝光不同的图像合并为一个可见度更好的图像。 （《Recovering high dynamic range radiance maps from photographs》、《Exposure fusion: A simple and practical alternative to high dynamic range photography》）</p>
<p>​		该文采用的虚拟图像数据集 (VIDIT)  （《Vidit: Virtual image dataset for illumination transfer》）中 包含来自 390 个场景的图像，每个场景都使用八个不同的光方向和五个色温捕获 40 次，不同的光线方向会在不同的位置投射阴影，这使得通过选择非阴影像素来构建无阴影图像成为可能。</p>
<p>​		 该文采用和《Exposure fusion: A simple and practical alternative to high dynamic range photography》相同的策略来构建由 OpenCV 包 （《 The OpenCV Library. Dr. Dobb’s Journal of Software Tools》）实现的无阴影图像：</p>
<p>（1）太暗（曝光不足）或太亮（曝光过度）的像素被赋予较小的权重。 </p>
<p>（2）具有高饱和度（RGB 通道的标准偏差）的像素通常处于良好的光照下，被赋予较大的权重。  </p>
<p>（3）边缘和纹理通常包含更多信息，被认为更重要。 </p>
<p>​		下图是一个曝光融合的例子。图中的图像是在不同的光方向和色温下捕获的。 很明显，这些图像包含由点光源引起的阴影。 使用曝光融合方法后，得到一张场景结构明显的无阴影图像。</p>
<p>​	![](&#x2F;img-post&#x2F;论文分享&#x2F;2022-03-18-用于图像光源操作的深度重光照网络&#x2F;曝光融合方法.png)</p>
<p><img src="/../img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E6%9B%9D%E5%85%89%E8%9E%8D%E5%90%88%E6%96%B9%E6%B3%95.png"></p>
<h3 id="（3）对抗学习（Adversarial-Learning）"><a href="#（3）对抗学习（Adversarial-Learning）" class="headerlink" title="（3）对抗学习（Adversarial Learning）"></a>（3）对抗学习（<strong>Adversarial Learning</strong>）</h3><p>​		为了训练场景重建网络，通过卷积层（图中“Conv.”）形成无阴影图像。它将潜在结构传输回图像空间。 但是，阴影会导致输入图像中出现空洞。 为了填补具有良好感知一致性的漏洞，该文附加了一个鉴别器来辅助场景重建网络的训练，该文采用与 《Image-to-image translation with conditional adversarial networks》相同的鉴别器结构，该结构堆叠四个跨度卷积层，分层提取全局表示。 在训练过程中，判别器被分配来区分（场景重建网络的）估计与真实无阴影图像。 一开始，估计缺乏结构信息。 鉴别器注意到弱点并基于它进行分类。同时，场景重建网络被分配来欺骗鉴别器，预测与目标无阴影图像相似的结构相关性的估计图像。 在数学上，对抗性学习是：<br>$$<br>\mathcal{L}<em>{c G A N}(G, D)&#x3D;\mathbb{E}</em>{\left(\mathbf{X}, \mathbf{Y}<em>{s f}\right)}\left[\log D\left(\mathbf{X}, \mathbf{Y}</em>{s f}\right)\right]+\mathbb{E}_{\mathbf{X}}[\log (1-D(\mathbf{X}, G(\mathbf{X})))]<br>$$</p>
<p>​		其中生成器G 旨在最小化$\mathcal{L}<em>{c G A N}(G, D)$ 的损失，即 G* &#x3D; arg minG maxD LcGAN (G, D)。 鉴别器 D 试图最大化损失$\mathcal{L}</em>{c G A N}(G, D)$ 。 术语 $c G A N$ 表示鉴别器将输入图像 X 作为先验信息，是一种条件 GAN 结构。 考虑到估计的场景结构应该接近真实的无阴影目标 $\mathbf{Y}<em>{s f}$，使用传统的 L1 范数损失来衡量估计的每像素误差。 场景重建网络的目标定义为：<br>$$<br>G^{*}&#x3D;\lambda \mathbb{E}</em>{\left(\mathbf{X}, \mathbf{Y}<em>{s f}\right)}\left[\left|\mathbf{Y}</em>{s f}-G(\mathbf{X})\right|\right]+\arg \min _{G} \max <em>{D} \mathcal{L}</em>{c G A N}(G, D)<br>$$<br>其中术语$\lambda$ 平衡了 L1 范数和对抗性损失。</p>
<h2 id="3、阴影先验估计（Shadow-Prior-Estimation）"><a href="#3、阴影先验估计（Shadow-Prior-Estimation）" class="headerlink" title="3、阴影先验估计（Shadow Prior Estimation）"></a>3、阴影先验估计（Shadow Prior Estimation）</h2><p>​		不同的光源会产生不同的光效，例如产生不同的阴影和色温。 为了产生目标光源的光效，该文使用了与场景重建类似的网络结构来设计一个阴影先验估计网络，其架构如下图所示。</p>
<p>​	![](&#x2F;img-post&#x2F;论文分享&#x2F;2022-03-18-用于图像光源操作的深度重光照网络&#x2F;阴影先验估计网络.png)</p>
<p><img src="/../img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E9%98%B4%E5%BD%B1%E5%85%88%E9%AA%8C%E4%BC%B0%E8%AE%A1%E7%BD%91%E7%BB%9C.png"></p>
<p>​		该网络与场景重建网络的不同点主要有三个：</p>
<p>1）该网络丢弃了跳跃连接，因为该网络希望更加关注全局光效应，跳跃连接直接将局部特征带到输出中，这使网络不容易学习全局变化。</p>
<p>2）该网络使用经过修改的阴影区域鉴别器，以专注于阴影区域的识别。该鉴别器采用与（ 2017年《Image-to-image translation with conditional adversarial networks》 ）相同的结构，堆叠四个步幅卷积层，逐渐提取全局特征表示。 </p>
<p>​		为了关注阴影区域，首先校正估计以通过 z &#x3D; min(α; x) 关注低强度区域（暗，通常是阴影），其中符号 x 表示估计的像素强度。符号z表示将输入到鉴别器的修正值。术语 α 是阴影敏感度的预定义阈值（根据经验，它设置为 0.059 &#x3D; 15&#x2F;255）</p>
<p>  3）在数学上，阴影先验估计网络的目标可以描述如下：</p>
<p>$$<br>\begin{array}{r}<br>G^{*}&#x3D;\lambda \mathbb{E}_{\mathbf{X}, \mathbf{Y}}[|\mathbf{Y}-G(\mathbf{X})|]+\arg \min <em>{G} \max <em>{D} \mathcal{L}</em>{c G A N}(G, D) \<br>+\arg \min <em>{G} \max <em>{D</em>{s h a d}} \mathcal{L}</em>{c G A N}\left(G, D</em>{s h a d}\right)<br>\end{array}<br>$$<br>​		其中 ground-truth目标是目标光源下的图像。 Dshad 表示阴影区域鉴别器（细节将在下面说明）， Y 表示目标光源下的图像。</p>
<h2 id="4、重渲染模块"><a href="#4、重渲染模块" class="headerlink" title="4、重渲染模块"></a>4、重渲染模块</h2><pre><code>     经过场景重建网络和阴影先验估计网络的处理后，估计的场景结构和光照效果将融合在一起以产生重光照后的输出，如下图所示。
</code></pre>
<p>​	![](&#x2F;img-post&#x2F;论文分享&#x2F;2022-03-18-用于图像光源操作的深度重光照网络&#x2F;重渲染模块.png)</p>
<p><img src="/../img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E9%87%8D%E6%B8%B2%E6%9F%93%E6%A8%A1%E5%9D%97.png"></p>
<p>​		重新渲染器由三部分组成：多尺度感知、通道重新校准和绘制过程。 </p>
<p><strong>（1）多尺度感知块</strong>。重新渲染器使用一种新的多尺度感知块，不同尺度的感知块有利于获取场景的全局和局部信息，这是十分必要的，全局信息有利于阴影和光照的一致性，而局部信息增强了光照效果的纹理细节。该文使用的多尺度感知块具有不同感知大小的过滤器（例如，过滤器大小为 3 × 3、5 × 5，…）。</p>
<p>（2）<strong>通道重新校准。</strong>该过程用于学习不同通道的权重，从而为以下绘制过程选择关键特征。经过多尺度感知块处理后，具有不同空间感知的特征被合并成一个单一的特征图，其中每个通道存储一种类型的空间模式。 但是，不同的模式对于重新渲染过程可能具有不同的重要性。 </p>
<p>（3）<strong>绘制块。</strong>绘制块是一个卷积层（滤波器大小为 7 × 7，填充为 3，步幅为 1 和 tanh 激活函数），用于将预测特征从特征空间绘制到图像空间。</p>
<p>​		<strong>损失函数</strong>：重渲染器的损失函数由每像素重建误差和感知误差组成。 重建误差是通过广泛使用的 L1 范数损失来衡量的。感知相似度（2016年的《Perceptual losses for real-time style transfer and super-resolution》）基于从 VGG-19 网络中提取的特征计算。 VGG-19网络使用 ImageNet 数据集进行了预训练，用于图像分类。 提取的特征对视觉比较具有判别力，因此可以用来衡量感知相似度。 损失函数定义为：<br>$$<br>\mathcal{L}(\mathbf{Y}, \hat{\mathbf{Y}})&#x3D;|\mathbf{Y}-\hat{\mathbf{Y}}|+\lambda|f \operatorname{eat}(\mathbf{Y})-\operatorname{feat}(\hat{\mathbf{Y}})|<br>$$<br>​		其中平衡系统设置为0.01。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><h2 id="1、数据集"><a href="#1、数据集" class="headerlink" title="1、数据集"></a>1、数据集</h2><p>​        Virtual Image Dataset for Illumination Transfer (VIDIT) 数据集包含 390 个不同场景内容（例如金属、木材等）的虚拟场景，其中有 300 个用于训练的场景、45 个用于验证的场景和 45 个用于单独测试场景 。 所有场景由强大的游戏引擎（虚幻引擎 4 ）渲染以获得高分辨率图像。  VIDIT 数据集的目标是照明操作。 每个场景用八个光方向和五个色温进行渲染，从而产生分辨率为 1024*1024 的 40 张图像。该文使用 300 个训练场景中的所有可能对来训练网络，并使用提供的验证数据集（45 个场景）进行评估。</p>
<h2 id="2、训练策略"><a href="#2、训练策略" class="headerlink" title="2、训练策略"></a>2、训练策略</h2><p>​		受 GPU 内存和计算能力的限制，该文对三个子网络（场景重建、阴影先验估计和重新渲染器）进行分别训练。 首先，作者通过设计的损失函数使用成对的输入图像和无阴影目标图像来训练场景重建网络。 接着，使用成对的输入图像和目标图像来训练阴影先验估计网络。 最后，在固定场景重建网络和阴影先验估计网络并移除了它们的最后一个卷积层和鉴别器（图 5 中的绿色圆圈和图 6 中的粉红色圆圈）的情况下，使用设计的损失函数训练重新渲染器网络。</p>
<h2 id="3、训练设置"><a href="#3、训练设置" class="headerlink" title="3、训练设置"></a>3、训练设置</h2><p>​		所有训练图像的大小从 <code>1024*1024</code> 调整为 <code>512*512</code>，并且 mini-batch 大小设置为 6。 使用Adam优化方法，动量为0.5，学习率为0.0001。 网络随机初始化为[33]。 正如前文提到的，场景重建网络和阴影先验估计网络首先独立训练，其中每个网络训练 20 个epoch。 然后，将两个网络固定，重新渲染器网络也训练了 20 个 epoch。 所有实验都是通过 PyTorch （2019年的《Pytorch: An imperative style, high-performance deep learning library》）在带有两个 NVIDIA GTX2080Ti GPU 的 PC 上进行的。</p>
<h2 id="4、评价方法"><a href="#4、评价方法" class="headerlink" title="4、评价方法"></a>4、评价方法</h2><p>​		本文使用峰值信噪比（PSNR）和结构相似性（SSIM）（2004年的《Image quality assessment: from error visibility to structural similarity》）来衡量预测图像与真实图像之间的相似性，其中较大的值意味着更好的性能。 为了衡量感知质量，本文使用了学习感知图像块相似度（LPIPS）（2020年的《Bisenet v2: Bilateral network with guided aggregation for real-time semantic segmentation》），其中较小的值意味着更多的感知相似度。</p>
<h2 id="5、消融实验"><a href="#5、消融实验" class="headerlink" title="5、消融实验"></a>5、消融实验</h2><p>​		由于基于深度学习的重光照是一项新话题，因此可用于比较的方法很少。重光照问题可以看作是将光源转换为目标设置的图像到图像转换任务，因此，该文章采用了Pix2Pix[14]  作为基线模型。 Pix2pix 是一种通过对抗策略训练的有条件 GAN 结构，在图像到图像的转换任务中取得了巨大的成功，如背景去除、姿势转移等。Pix2Pix 方法基于自动编码器结构，其中输入图像首先被下采样四次（比例缩小到 1&#x2F;16），然后由九个残差块处理。 最后，一组反卷积层用于将图像上采样回原始大小并形成估计。 </p>
<p>​	![](&#x2F;img-post&#x2F;论文分享&#x2F;2022-03-18-用于图像光源操作的深度重光照网络&#x2F;不同结构的比较.png)</p>
<p><img src="/../img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E4%B8%8D%E5%90%8C%E7%BB%93%E6%9E%84%E7%9A%84%E6%AF%94%E8%BE%83.png"></p>
<p>​		表 1 给出了不同结构之间的比较，其中 ShadAdv 和 BPAE 是 Pxi2Pix 网络的两种变体。 基线方法 (Pix2Pix) 在 PSNR 中达到 16.28 dB，在 SSIM 中达到 0.553，在 LPIPS 中达到 0.482。</p>
<h4 id="（A）Pix2Pix和ShadAdv的比较"><a href="#（A）Pix2Pix和ShadAdv的比较" class="headerlink" title="（A）Pix2Pix和ShadAdv的比较"></a>（A）Pix2Pix和ShadAdv的比较</h4><p>​		ShadAdv和 Pix2Pix的区别就是ShadAdv添加了提出的阴影区域鉴别器来增强基线 Pix2Pix。 表中PSNR 提高了 0.84(&#x3D; 17.12 − 16.28) dB，感知质量在 LPIPS 方面提高了 0.042 (&#x3D; 0.482 − 0.440)。可以得出结论，与原始的 Pix2Pix 方法相比，“ShadAdv”更加注重阴影区域的外观。 换句话说，阴影鉴别器可以为重新投射目标光源的阴影提供更好的指导。</p>
<h4 id="（B）反投影模块的效果"><a href="#（B）反投影模块的效果" class="headerlink" title="（B）反投影模块的效果"></a>（B）反投影模块的效果</h4><p>​		“Pix2Pix”和“ShadAdv”方法基于自动编码器结构。 正如我们所提到的，它通过堆叠的卷积和反卷积层对图像进行下采样和上采样。  “BPAE”方法是自动编码器的增强版本，其中下采样和上采样过程由 DBP 和 UBP 块完成（如图 4 所示）。  BP 块基于反投影理论，它弥补了下采样和上采样过程中丢失的信息。 与自动编码器结构（在“ShadAdv”中使用）相比，“BPAE”方法提取了更多信息特征，丰富了估计的结构，将 SSIM 从 0.569 增加到 0.573。</p>
<h4 id="（C）重光照假设的影响"><a href="#（C）重光照假设的影响" class="headerlink" title="（C）重光照假设的影响"></a>（C）重光照假设的影响</h4><p>​		Pix2Pix、ShadAdv和BPAE方法都直接学习输入图像到真实图像的映射，本文所提出的方法DRN则使用了两阶段假设，以最高的 PSNR (17.59 dB) 和 SSIM (0.596) 分数以及可比较的视觉相似性（LPIPS 的 0.440）实现了最佳重建。</p>
<p>​		该文将AnyToOne重照明任务视为两个阶段问题，其中第一阶段找到来自输入图像 X的场景结构 L−1 Φ (X) 和光效 L(Φ→Ψ)(X) 。第二阶段在目标光源下绘制 P(·) 估计 ˆY。   “DRN”是基于我们的重照明假设提出的方法。 </p>
<h2 id="6、对比实验"><a href="#6、对比实验" class="headerlink" title="6、对比实验"></a>6、对比实验</h2><p>​		除了与Pix2Pix进行对比，该文还与其他代表性方法进行了比较。 该文在 VIDIT训练数据集上用原方法的设置重新训练，并使用 VIDIT 验证数据集进行了比较。</p>
<p>​		 U-Net [22] 是一个流行的 CNN 结构，最初设计用于生物医学图像分割。 它由下采样（编码器）和上采样（解码器）路径组成，形成一个自动编码器结构，其中几个短接将信息从编码器直接传输到解码器部分。	</p>
<p>​		Retinex-Net [35] 旨在启发基于 Retinex 理论的低光图像， 它首先将低光图像分解为反射率和照明元素，然后调整子网络细化照明以照亮输入图像。 </p>
<p>​	![](&#x2F;img-post&#x2F;论文分享&#x2F;2022-03-18-用于图像光源操作的深度重光照网络&#x2F;定量对比结果.png)</p>
<p><img src="/../img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E5%AE%9A%E9%87%8F%E5%AF%B9%E6%AF%94%E7%BB%93%E6%9E%9C.png"></p>
<p>​		接下来的定性对比实验结果：</p>
<p>![](&#x2F;img-post&#x2F;论文分享&#x2F;2022-03-18-用于图像光源操作的深度重光照网络&#x2F;定性对比结果.png)</p>
<p><img src="/../img-post/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/2022-03-18-%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E6%93%8D%E4%BD%9C%E7%9A%84%E6%B7%B1%E5%BA%A6%E9%87%8D%E5%85%89%E7%85%A7%E7%BD%91%E7%BB%9C/%E5%AE%9A%E6%80%A7%E5%AF%B9%E6%AF%94%E7%BB%93%E6%9E%9C.png"></p>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB/" rel="tag"># 论文分享</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2021-11-26-%E3%80%90%E7%A7%91%E7%A0%94%E7%AC%94%E8%AE%B0%E3%80%91GPU%E5%92%8CCPU%E7%9A%84%E5%88%A9%E7%94%A8%E7%8E%87/" rel="prev" title="「科研笔记」GPU和CPU的利用率">
                  <i class="fa fa-angle-left"></i> 「科研笔记」GPU和CPU的利用率
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2022-03-21-%E3%80%90%E8%AE%BA%E6%96%87%E5%88%86%E4%BA%AB%E3%80%91%E7%94%A8%E4%BA%8E%E5%9B%BE%E5%83%8F%E5%85%89%E6%BA%90%E8%BF%81%E7%A7%BB%E7%9A%84%E5%A4%9A%E5%B0%BA%E5%BA%A6%E8%87%AA%E6%A0%A1%E5%87%86%E7%BD%91%E7%BB%9C/" rel="next" title="「论文分享」用于图像光源迁移的多尺度自校准网络">
                  「论文分享」用于图像光源迁移的多尺度自校准网络 <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder">vvbuys</span>
  </div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.js.org/muse/" rel="noopener" target="_blank">NexT.Muse</a>
  </div>

    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="back-to-top" role="button" aria-label="Back to top">
    <i class="fa fa-arrow-up fa-lg"></i>
    <span>0%</span>
  </div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script>

  






  





</body>
</html>
